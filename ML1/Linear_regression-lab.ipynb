{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lab 2 - Linear regression </br> M1 Data Science, ML1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# \"IPython magic command\" to sutomatically reload the imported packages after changes in a package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# plt.rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"]})\n",
    "# plt.rc(\"text\", usetex=True)\n",
    "\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 24\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # default text size\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# display backend for matplotlib\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\" style=\"margin-top: 0px\">\n",
    "\n",
    "Lab report written by: **Danila Pechenev**, **Gwenn Garrigues**.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: linear regression\n",
    "\n",
    "Let $N, M$ be two postive integers, and consider a dataset $\\mathcal{D} = \\{ (y_n \\mathbf{x}_n) \\}_{1 \\leq n \\leq N}$, with targets $\\mathbf{y} = (y_n)_{1 \\leq n \\leq N} \\in \\mathbb{R}^N$, and (standardized) data $\\mathbf{X} = [\\mathbf{x}_1, \\dotsc, \\mathbf{x}_N]^T \\in \\mathbb{R}^{N \\times M}$.\n",
    "\n",
    "This exercise is aimed at performing linear regression with `numpy` and `sklearn` on the `California house` dataset, loaded below.\n",
    "\n",
    "For the sake of simplicity, the dataset is NOT split into a train and a test set until question 5, where the quality of the model is properly assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33:291-297, 1997.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# X = housing.data, y = housing.target\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "print(housing.feature_names)\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges: \n",
      " names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude'] \n",
      " dr.  : [1.45e+01 5.10e+01 1.41e+02 3.37e+01 3.57e+04 1.24e+03 9.41e+00 1.00e+01] \n",
      " min  : [   0.5     1.      0.85    0.33    3.      0.69   32.54 -124.35] \n",
      " max  : [ 1.50e+01  5.20e+01  1.42e+02  3.41e+01  3.57e+04  1.24e+03  4.20e+01\n",
      " -1.14e+02]\n"
     ]
    }
   ],
   "source": [
    "features_min = np.min(X, axis=0)\n",
    "features_max = np.max(X, axis=0)\n",
    "range_features = np.abs(features_max - features_min)\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(\n",
    "        \"Feature ranges: \\n names: {} \\n dr.  : {} \\n min  : {} \\n max  : {}\".format(\n",
    "            housing.feature_names, range_features, features_min, features_max\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "1. Standardize the data using `sklearn`, and store the output into a variable `scaled_X`. Is it useful for this dataset? Justify your answer.\n",
    "\n",
    "> Indication: take a look at [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.34476576,  0.98214266,  0.62855945, -0.15375759, -0.9744286 ,\n",
       "       -0.04959654,  1.05254828, -1.32783522])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled_X = scaler.transform(X)\n",
    "scaled_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 8px; background-color:#f0fff0\">\n",
    "We do that to avoid loss function dominated by some features (due to the different ranges of them). It is useful for this dataset because, as we've just seen, the columns of the dataset have different ranges. For example, the range of <b>Population</b> is 3.57e+04 and the range of <b>Latitude</b> is just 1.00e+01\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "2. Write ERM problem corresponding to linear least-squares regression. Recall the form of the analytic expression solution, with the definition of the elements involved.\n",
    "\n",
    "    > Indication: type your answer in $\\LaTeX$ in the cell below, or include a picture of your handwritten answer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 8px; background-color:#f0fff0\">\n",
    "\n",
    "$$\\beta^* \\in \\underset{\\beta\\in\\mathbb{R}^{D+1}}{argmin}\\frac{1}{2N}||y-\\tilde{X}\\beta||^2_2 \\text{, with } \\tilde{X} = \\left[ \\phi(x_1)^T, ... , \\phi(x_N)^T \\right]^T \\in \\mathbb{R}^{N \\times(D+1)}$$\n",
    "$$\\text{where }\\phi : R^D \\to R^{D+1} : x \\mapsto [1,x^T]^T.$$\n",
    "$$\\text{If rank}(\\tilde{X}) = D + 1, \\tilde{X}^T\\tilde{X} \\text{ is invertible, and}$$\n",
    "$$\\beta^* = (\\tilde{X}^T\\tilde{X})^{-1}\\tilde{X}^Ty$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "3. \n",
    "   1. Compute the solution to the problem with `numpy` from the analytic expression, calling the result `beta_numpy`.\n",
    "   \n",
    "   2. Compute the solution with `sklearn`, `beta_sklearn`. \n",
    "\n",
    "   3. Test whether the two array are significantly different. \n",
    "\n",
    "> Hints: \n",
    "> - the function [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve) and the class [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/linear_model.html) will be useful\n",
    "> - the function [`numpy.allclose`](https://numpy.org/doc/stable/reference/generated/numpy.allclose.html#numpy-allclose) can be useful to test equality between two arrays.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wave = np.c_[X, np.ones(X.shape[0])]\n",
    "beta_numpy = np.linalg.solve(np.transpose(X_wave) @ X_wave, np.transpose(X_wave) @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "beta_sklearn = np.append(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(beta_numpy, beta_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 8px; background-color:#f0fff0\">\n",
    "beta_numpy and beta_sklearn are the same\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "4. Evaluate $\\kappa(\\widetilde{\\mathbf{X}}^T\\widetilde{\\mathbf{X}})$, the condition number of the matrix $\\widetilde{\\mathbf{X}}^T\\widetilde{\\mathbf{X}}$ using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html#numpy.linalg.cond). Is the least-squares problem considered well-conditioned? Justify your answer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(56803951681.7832)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(np.transpose(X_wave) @ X_wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 8px; background-color:#f0fff0\">\n",
    "The matrix is very ill-conditioned because cond_number >> 1 (even more than $10^{10}$), and inversion is unreliable \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "5. To properly assess the quality of the regressor, the dataset will now be split into a train and a test set. Complete the code below to:\n",
    "    - split the dataset into a train and a test set\n",
    "    - use `sklearn` to apply K-fold cross validation on the train set, and return the regressor associated with the best MSE criterion;\n",
    "    - evaluate the performance on the test set in terms of the MSE criterion.\n",
    "\n",
    "    > Indications: \n",
    "    > - if needed, take a look at the documentation of [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#train-test-split), and take some inspiration from [`sklearn` examples](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py).\n",
    "    >\n",
    "    > - the documentation and examples for [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) (or [`sklearn.pipeline.make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)) and [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) can be useful;\n",
    "    >\n",
    "    > - to return the regression coefficients from the pipeline, take a look its [named_steps](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.named_steps) method (if needed, see also the [stackoverflow answer](https://stackoverflow.com/questions/43856280/return-coefficients-from-pipeline-object-in-sklearn));\n",
    "    >\n",
    "    > - for the evaluation metrics, see [the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html) and the module [sklearn.metrics](https://scikit-learn.org/stable/api/sklearn.metrics.html) \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: MSE: 0.52\n",
      "        MAE: 0.53\n",
      "        R2 : 0.62\n",
      "Fold 1: MSE: 0.50\n",
      "        MAE: 0.53\n",
      "        R2 : 0.61\n",
      "Fold 2: MSE: 0.52\n",
      "        MAE: 0.52\n",
      "        R2 : 0.61\n",
      "Fold 3: MSE: 0.51\n",
      "        MAE: 0.52\n",
      "        R2 : 0.61\n",
      "Fold 4: MSE: 0.55\n",
      "        MAE: 0.54\n",
      "        R2 : 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# Create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"linearregression\", linear_regression)])\n",
    "\n",
    "# Set-up cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "fold_id = 0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_subtrain, X_subtest = X_train[train_index], X_train[test_index]\n",
    "    y_subtrain, y_subtest = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # train the model using the scaled train set\n",
    "    pipe.fit(X_subtrain, y_subtrain)\n",
    "\n",
    "    # make predictions using the test set\n",
    "    y_pred = pipe.predict(X_subtest)\n",
    "\n",
    "    # display mean squared error\n",
    "    print(\"Fold {}: MSE: {:.2f}\".format(fold_id, mean_squared_error(y_subtest, y_pred)))\n",
    "    # display mean absolute error\n",
    "    print(\"        MAE: {:.2f}\".format(mean_absolute_error(y_subtest, y_pred)))\n",
    "    # display coefficient of determination: 1 is perfect prediction\n",
    "    print(\"        R2 : {:.2f}\".format(r2_score(y_subtest, y_pred)))\n",
    "    fold_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the test set: 0.5558915986952442\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "print(f\"MSE on the test set: {mean_squared_error(y_test, pipe.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: least-squares and matrix inversion\n",
    "\n",
    "Let $N, M$ be two positive integers, $\\mathbf{A} \\in \\mathbb{R}^{N \\times M}$ such that $\\text{rank}(\\mathbf{A}) = M$, and $\\mathbf{y} \\in \\mathbb{R}^N$. \n",
    "\n",
    "This exercise is aimed at comparing the accuracy and speed of 2 approaches to compute $\\widehat{\\boldsymbol{\\beta}} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y}$, solution to the problem \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\underset{\\boldsymbol{\\beta} \\in \\mathbb{R}^M}{\\text{minimize}} \\; \\frac{1}{2} \\| \\mathbf{y} - \\mathbf{A}\\boldsymbol{\\beta} \\|_2^2\n",
    "\\end{equation*}\n",
    "\n",
    "1. computing $\\mathbf{B} = (\\mathbf{A}^T\\mathbf{A})^{-1}$, and then $\\widehat{\\boldsymbol{\\beta}} = \\mathbf{B}\\mathbf{A}^T \\mathbf{y}$.\n",
    "2. solving the problem with an iterative solver, e.g., using [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve).\n",
    "\n",
    "To do so, we will use the data generated below, knowing the ground truth value $\\boldsymbol{\\beta}^*$ to evaluate numerical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)\n",
    "\n",
    "N = 200\n",
    "M = 100\n",
    "\n",
    "A = rng.standard_normal(size=(N, M))\n",
    "beta_true = rng.standard_normal(size=(M,))\n",
    "y = A @ beta_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "1. \n",
    "   - Compute $\\widehat{\\boldsymbol{\\beta}}$ with the two approaches mentioned above, registering the time needed to solve the problem with the 2 version. To do so, complete the code cell below.\n",
    "   - Compare the time required between the two approaches\n",
    "\n",
    "> Indication: see the functions [`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html#numpy.linalg.inv) and [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 μs ± 362 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Approach based on direct matrix inversion\n",
    "\n",
    "B = np.linalg.inv(np.transpose(A) @ A)\n",
    "beta_inv = B @ np.transpose(A) @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 μs ± 22.9 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Approach based on iterative algorithm\n",
    "\n",
    "beta_solve = np.linalg.solve(np.transpose(A) @ A, np.transpose(A) @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.linalg.inv(np.transpose(A) @ A)\n",
    "beta_inv = B @ np.transpose(A) @ y\n",
    "\n",
    "beta_solve = np.linalg.solve(np.transpose(A) @ A, np.transpose(A) @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "2. The accuracy of $\\widehat{\\boldsymbol{\\beta}}$ can be assess with respect to the ground truth $\\boldsymbol{\\beta}^*$ in terms of the reconstruction signal to noise ratio (rSNR), expressed in dB, defined by\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{rSNR}(\\widehat{\\boldsymbol{\\beta}}, \\boldsymbol{\\beta}^*) = 10 \\log_{10} \\Big( \\frac{\\| \\boldsymbol{\\beta}^* \\|_2^2}{\\| \\boldsymbol{\\beta}^* - \\widehat{\\boldsymbol{\\beta}} \\|_2^2} \\Big).\n",
    "\\end{equation*}\n",
    "\n",
    "- Compute the rSNR for the solution `beta_inv` and `beta_solve` computed above, and indicate which of the two esimates is the most precise.\n",
    "- Conclude on the best approach to adopt to solve linear systems of equations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsnb_beta_inv = 10 * np.log10(np.square(beta_true).sum() / np.square(beta_true - beta_inv).sum())\n",
    "rsnb_beta_solve = 10 * np.log10(np.square(beta_true).sum() / np.square(beta_true - beta_solve).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rSNR\n",
      "beta_inv: 296.72178630583676, beta_solve: 295.85707005005355\n"
     ]
    }
   ],
   "source": [
    "print('rSNR')\n",
    "print(f\"beta_inv: {rsnb_beta_inv}, beta_solve: {rsnb_beta_solve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 8px; background-color:#f0fff0\">\n",
    "We can see that the approaches are almost equally precise (the difference is negligible)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 8px; background-color:#f0fff0\">\n",
    "The approach using numpy.linalg.solve is better than the one using np.linalg.inv because it works faster (and even a little bit more precise)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "3. Use `numpy` to compute the condition number $\\kappa(\\mathbf{A})$ of the matrix $\\mathbf{A}$. What information does this value convey about the inversion problem considered?\n",
    "\n",
    "> Indication: take a look at the documentation of the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html#numpy.linalg.cond).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.3385724263555785)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 8px; background-color:#f0fff0\">\n",
    "A is ~5 so it is considered to be well-conditioned. That means that any small perturbation, for instance, 1% error in data, could at most cause about 5% error in the solution because the relative error in the solution is bounded by the condition number times the relative error in the data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "End of lab2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "title": "TP1 TSI : Introduction au filtrage numérique"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
